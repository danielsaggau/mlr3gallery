---
title: House prices in King Country
author: Florian Pfisterer
date: '2020-01-30'
slug: house-prices-in-king-country
categories: []
tags: ['regression', 'visualization', 'random forest', 'feature engineering', 'tuning', 'mlr3tuning']
---



<p>This use case shows how to model housing price data in King Country.
Following features are illustrated:</p>
<ul>
<li>Summarizing the data set</li>
<li>Converting data to treat it as a numeric feature/factor</li>
<li>Generating new variables</li>
<li>Splitting data into train and test data sets</li>
<li>Computing a first model (decision tree)</li>
<li>Building many trees (random forest)</li>
<li>Visualizing price data across different region</li>
<li>Optimizing the baseline by implementing a tuner</li>
<li>Engineering features</li>
<li>Creating a sparser model</li>
</ul>
<div id="use-case-regr-houses" class="section level2">
<h2>House Price Prediction in King County</h2>
<p>We use the <code>house_sales_prediction</code> dataset contained in the package <a href="https://mlr3book.mlr-org.com">mlr3book</a> in order to provide a use-case for the application of <a href="https://mlr3.mlr-org.com">mlr3</a> on real-world data.</p>
<pre class="r"><code>library(mlr3)
library(mlr3book)
data(&quot;house_sales_prediction&quot;, package = &quot;mlr3book&quot;)</code></pre>
<div id="exploratory-data-analysis" class="section level3">
<h3>Exploratory Data Analysis</h3>
<p>In order to get a quick impression of our data, we perform some initial <em>Exploratory Data Analysis</em>.
This helps us to get a first impression of our data and might help us arrive at additional features that can help with the prediction of the house prices.</p>
<p>We can get a quick overview using R’s summary function:</p>
<pre class="r"><code>summary(house_sales_prediction)</code></pre>
<pre><code>##        id        date               price            bedrooms     
##  Min.   :0   Length:21613       Min.   :  75000   Min.   : 0.000  
##  1st Qu.:0   Class :character   1st Qu.: 321950   1st Qu.: 3.000  
##  Median :0   Mode  :character   Median : 450000   Median : 3.000  
##  Mean   :0                      Mean   : 540088   Mean   : 3.371  
##  3rd Qu.:0                      3rd Qu.: 645000   3rd Qu.: 4.000  
##  Max.   :0                      Max.   :7700000   Max.   :33.000  
##    bathrooms      sqft_living       sqft_lot           floors     
##  Min.   :0.000   Min.   :  290   Min.   :    520   Min.   :1.000  
##  1st Qu.:1.750   1st Qu.: 1427   1st Qu.:   5040   1st Qu.:1.000  
##  Median :2.250   Median : 1910   Median :   7618   Median :1.500  
##  Mean   :2.115   Mean   : 2080   Mean   :  15107   Mean   :1.494  
##  3rd Qu.:2.500   3rd Qu.: 2550   3rd Qu.:  10688   3rd Qu.:2.000  
##  Max.   :8.000   Max.   :13540   Max.   :1651359   Max.   :3.500  
##    waterfront            view          condition         grade       
##  Min.   :0.000000   Min.   :0.0000   Min.   :1.000   Min.   : 1.000  
##  1st Qu.:0.000000   1st Qu.:0.0000   1st Qu.:3.000   1st Qu.: 7.000  
##  Median :0.000000   Median :0.0000   Median :3.000   Median : 7.000  
##  Mean   :0.007542   Mean   :0.2343   Mean   :3.409   Mean   : 7.657  
##  3rd Qu.:0.000000   3rd Qu.:0.0000   3rd Qu.:4.000   3rd Qu.: 8.000  
##  Max.   :1.000000   Max.   :4.0000   Max.   :5.000   Max.   :13.000  
##    sqft_above   sqft_basement       yr_built     yr_renovated   
##  Min.   : 290   Min.   :   0.0   Min.   :1900   Min.   :   0.0  
##  1st Qu.:1190   1st Qu.:   0.0   1st Qu.:1951   1st Qu.:   0.0  
##  Median :1560   Median :   0.0   Median :1975   Median :   0.0  
##  Mean   :1788   Mean   : 291.5   Mean   :1971   Mean   :  84.4  
##  3rd Qu.:2210   3rd Qu.: 560.0   3rd Qu.:1997   3rd Qu.:   0.0  
##  Max.   :9410   Max.   :4820.0   Max.   :2015   Max.   :2015.0  
##     zipcode           lat             long        sqft_living15 
##  Min.   :98001   Min.   :47.16   Min.   :-122.5   Min.   : 399  
##  1st Qu.:98033   1st Qu.:47.47   1st Qu.:-122.3   1st Qu.:1490  
##  Median :98065   Median :47.57   Median :-122.2   Median :1840  
##  Mean   :98078   Mean   :47.56   Mean   :-122.2   Mean   :1987  
##  3rd Qu.:98118   3rd Qu.:47.68   3rd Qu.:-122.1   3rd Qu.:2360  
##  Max.   :98199   Max.   :47.78   Max.   :-121.3   Max.   :6210  
##    sqft_lot15    
##  Min.   :   651  
##  1st Qu.:  5100  
##  Median :  7620  
##  Mean   : 12768  
##  3rd Qu.: 10083  
##  Max.   :871200</code></pre>
<pre class="r"><code>dim(house_sales_prediction)</code></pre>
<pre><code>## [1] 21613    21</code></pre>
<p>Our dataset has 21613 observations and 21 columns.
The variable we want to predict is <code>price</code>.
In addition to the price column, we have several other columns:</p>
<ul>
<li><p><code>id:</code> A unique identifier for every house.</p></li>
<li><p><code>date</code>: A date column, indicating when the house was sold.
This column is currently not encoded as a <code>date</code> and requires some preprocessing.</p></li>
<li><p><code>zipcode</code>: A column indicating the ZIP code. This is a categorical variable with many factor levels.</p></li>
<li><p><code>long, lat</code> The longitude and latitude of the house</p></li>
<li><p><code>...</code> several other numeric columns providing information about the house, such as number of rooms, square feet etc.</p></li>
</ul>
<p>Before we continue with the analysis, we preprocess some features so that they are stored in the correct format.</p>
<p>First we convert the <code>date</code> column to <code>numeric</code> to be able to treat it as a numeric feature:</p>
<pre class="r"><code>library(lubridate)
house_sales_prediction$date = ymd(substr(house_sales_prediction$date, 1, 8))
house_sales_prediction$date = as.numeric(as.Date(house_sales_prediction$date, origin = &quot;1900-01-01&quot;))
house_sales_prediction$date = house_sales_prediction$date</code></pre>
<p>Afterwards, we convert the zip code to a factor:</p>
<pre class="r"><code>house_sales_prediction$zipcode = as.factor(house_sales_prediction$zipcode)</code></pre>
<p>And add a new column <strong>renovated</strong> indicating whether a house was renovated at some point.</p>
<pre class="r"><code>house_sales_prediction$renovated = as.numeric(house_sales_prediction$yr_renovated &gt; 0)
# And drop the id column:
house_sales_prediction$id = NULL</code></pre>
<p>Additionally we convert the price from Dollar to units of 1000 Dollar to improve readability.</p>
<pre class="r"><code>house_sales_prediction$price = house_sales_prediction$price / 1000</code></pre>
<p>We can now plot the density of the <strong>price</strong> to get a first impression on its distribution.</p>
<pre class="r"><code>library(ggplot2)
ggplot(house_sales_prediction, aes(x = price)) + geom_density()</code></pre>
<p><img src="/post/2020-01-30-house-prices-in-king-country_files/figure-html/09-use-cases-007-1.png" width="672" /></p>
<p>We can see that the prices for most houses lie between 75.000 and 1.5 million dollars.
There are few extreme values of up to 7.7 million dollars.</p>
<p>Feature engineering often allows us to incorporate additional knowledge about the data and underlying processes.
This can often greatly enhance predictive performance.
A simple example: A house which has <code>yr_renovated == 0</code> means that is has not been renovated yet.
Additionally we want to drop features which should not have any influence (<code>id column</code>).</p>
<p>After those initial manipulations, we load all required packages and create a Task containing our data.</p>
<pre class="r"><code>library(mlr3)
library(mlr3viz)
tsk = TaskRegr$new(&quot;sales&quot;, house_sales_prediction, target = &quot;price&quot;)</code></pre>
<p>We can inspect associations between variables using <code>mlr3viz</code>’s <code>autoplot</code> function in order to get some good first impressions for our data.
Note, that this does in no way prevent us from using other powerful plot functions of our choice on the original data.</p>
<div id="distribution-of-the-price" class="section level4">
<h4>Distribution of the price:</h4>
<p>The outcome we want to predict is the <strong>price</strong> variable.
The <code>autoplot</code> function provides a good first glimpse on our data.
As the resulting object is a <code>ggplot2</code> object, we can use <code>faceting</code> and other functions from <strong>ggplot2</strong> in order to enhance plots.</p>
<pre class="r"><code>library(ggplot2)
autoplot(tsk) + facet_wrap(~renovated)</code></pre>
<p><img src="/post/2020-01-30-house-prices-in-king-country_files/figure-html/09-use-cases-009-1.png" width="672" /></p>
<p>We can observe that renovated flats seem to achieve higher sales values, and this might thus be a relevant feature.</p>
<p>Additionally, we can for example look at the condition of the house.
Again, we clearly can see that the price rises with increasing condition.</p>
<pre class="r"><code>autoplot(tsk) + facet_wrap(~condition)</code></pre>
<p><img src="/post/2020-01-30-house-prices-in-king-country_files/figure-html/09-use-cases-010-1.png" width="672" /></p>
</div>
<div id="association-between-variables" class="section level4">
<h4>Association between variables</h4>
<p>In addition to the association with the target variable, the association between the features can also lead to interesting insights.
We investigate using variables associated with the quality and size of the house.
Note that we use <code>$clone()</code> and <code>$select()</code> to clone the task and select only a subset of the features for the <code>autoplot</code> function, as <code>autoplot</code> per default uses all features.
The task is cloned before we select features in order to keep the original task intact.</p>
<pre class="r"><code># Variables associated with quality
autoplot(tsk$clone()$select(tsk$feature_names[c(3, 17)]),
  type = &quot;pairs&quot;)</code></pre>
<pre><code>## Registered S3 method overwritten by &#39;GGally&#39;:
##   method from   
##   +.gg   ggplot2</code></pre>
<p><img src="/post/2020-01-30-house-prices-in-king-country_files/figure-html/09-use-cases-011-1.png" width="672" /></p>
<pre class="r"><code>autoplot(tsk$clone()$select(tsk$feature_names[c(9:12)]),
  type = &quot;pairs&quot;)</code></pre>
<p><img src="/post/2020-01-30-house-prices-in-king-country_files/figure-html/09-use-cases-012-1.png" width="672" /></p>
</div>
</div>
<div id="splitting-into-train-and-test-data" class="section level3">
<h3>Splitting into train and test data</h3>
<p>In <code>mlr3</code>, we do not create <code>train</code> and <code>test</code> data sets, but instead keep only a vector of train and test indices.</p>
<pre class="r"><code>set.seed(4411)
train.idx = sample(seq_len(tsk$nrow), 0.7 * tsk$nrow)
test.idx = setdiff(seq_len(tsk$nrow), train.idx)</code></pre>
</div>
<div id="a-first-model-decision-tree" class="section level3">
<h3>A first model: Decision Tree</h3>
<p>Decision trees cannot only be used as a powerful tool for predictive models but also for exploratory data analysis.
In order to fit a decision tree, we first get the <code>regr.rpart</code> learner from the <code>mlr_learners</code> dictionary by using the sugar function <a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html"><code>lrn</code></a>.</p>
<p>For now we leave out the <code>zipcode</code> variable, as we also have the <code>latitude</code> and <code>longitude</code> of each house.</p>
<pre class="r"><code>tsk_nozip = tsk$clone()$select(setdiff(tsk$feature_names, &quot;zipcode&quot;))
# Get the learner
lrn = lrn(&quot;regr.rpart&quot;)
# And train on the task
lrn$train(tsk_nozip, row_ids = train.idx)</code></pre>
<pre class="r"><code>plot(lrn$model)
text(lrn$model)</code></pre>
<p><img src="/post/2020-01-30-house-prices-in-king-country_files/figure-html/09-use-cases-015-1.png" width="672" /></p>
<p>The learned tree relies on several variables in order to distinguish between cheaper and pricier houses.
The features we split along are <strong>grade</strong>, <strong>sqft_living</strong>, but also some features related to the area (longitude and latitude).</p>
<p>We can visualize the price across different regions in order to get more info:</p>
<pre class="r"><code># Load the ggmap package in order to visualize on a map
library(ggmap)

# And create a quick plot for the price
qmplot(long, lat, maptype = &quot;watercolor&quot;, color = log(price),
  data = house_sales_prediction[train.idx[1:3000],]) +
  scale_colour_viridis_c()</code></pre>
<p><img src="/post/2020-01-30-house-prices-in-king-country_files/figure-html/09-use-cases-016-1.png" width="672" /></p>
<pre class="r"><code># And the zipcode
qmplot(long, lat, maptype = &quot;watercolor&quot;, color = zipcode,
  data = house_sales_prediction[train.idx[1:3000],]) + guides(color = FALSE)</code></pre>
<p><img src="/post/2020-01-30-house-prices-in-king-country_files/figure-html/09-use-cases-016-2.png" width="672" /></p>
<p>We can see that the price is clearly associated with the zipcode when comparing then two plots.
As a result, we might want to indeed use the <strong>zipcode</strong> column in our future endeavours.</p>
</div>
<div id="a-first-baseline-decision-tree" class="section level3">
<h3>A first baseline: Decision Tree</h3>
<p>After getting an initial idea for our data, we might want to construct a first baseline, in order to see what a simple model already can achieve.</p>
<p>We use <code>resample</code> with <code>3-fold cross-validation</code> on our training data in order to get a reliable estimate of the algorithm’s performance on future data.
Before we start with defining and training learners, we create a <a href="https://mlr3.mlr-org.com/reference/Resampling.html"><code>Resampling</code></a> in order to make sure that we always compare on exactly the same data.</p>
<pre class="r"><code>library(mlr3learners)
cv3 = rsmp(&quot;cv&quot;, folds = 3)
cv3$instantiate(tsk$clone()$filter(train.idx))</code></pre>
<p>For the cross-validation we only use the <strong>training data</strong> by cloning the task and selecting only observations from the training set.</p>
<pre class="r"><code>lrn_rpart = lrn(&quot;regr.rpart&quot;)
res = resample(task = tsk$clone()$filter(train.idx), lrn_rpart, cv3)</code></pre>
<pre><code>## INFO  [13:34:09.492] Applying learner &#39;regr.rpart&#39; on task &#39;sales&#39; (iter 1/3) 
## INFO  [13:34:09.566] Applying learner &#39;regr.rpart&#39; on task &#39;sales&#39; (iter 2/3) 
## INFO  [13:34:09.620] Applying learner &#39;regr.rpart&#39; on task &#39;sales&#39; (iter 3/3)</code></pre>
<pre class="r"><code>res$score(msr(&quot;regr.mse&quot;))</code></pre>
<pre><code>##          task task_id            learner learner_id     resampling
##        &lt;list&gt;  &lt;char&gt;             &lt;list&gt;     &lt;char&gt;         &lt;list&gt;
## 1: &lt;TaskRegr&gt;   sales &lt;LearnerRegrRpart&gt; regr.rpart &lt;ResamplingCV&gt;
## 2: &lt;TaskRegr&gt;   sales &lt;LearnerRegrRpart&gt; regr.rpart &lt;ResamplingCV&gt;
## 3: &lt;TaskRegr&gt;   sales &lt;LearnerRegrRpart&gt; regr.rpart &lt;ResamplingCV&gt;
##    resampling_id iteration prediction regr.mse
##           &lt;char&gt;     &lt;int&gt;     &lt;list&gt;    &lt;num&gt;
## 1:            cv         1     &lt;list&gt; 45415.78
## 2:            cv         2     &lt;list&gt; 49598.67
## 3:            cv         3     &lt;list&gt; 42517.76</code></pre>
<pre class="r"><code>sprintf(&quot;RMSE of the simple rpart: %s&quot;, round(sqrt(res$aggregate()), 2))</code></pre>
<pre><code>## [1] &quot;RMSE of the simple rpart: 214.11&quot;</code></pre>
</div>
<div id="many-trees-random-forest" class="section level3">
<h3>Many Trees: Random Forest</h3>
<p>We might be able to improve upon the <strong>RMSE</strong> using more powerful learners.
We first load the <code>mlr3learners</code> package, which contains the <strong>ranger</strong> learner (a package which implements the “Random Forest” algorithm).</p>
<pre class="r"><code>lrn_ranger = lrn(&quot;regr.ranger&quot;, num.trees = 15L)
res = resample(task = tsk$clone()$filter(train.idx), lrn_ranger, cv3)</code></pre>
<pre><code>## INFO  [13:34:09.759] Applying learner &#39;regr.ranger&#39; on task &#39;sales&#39; (iter 1/3) 
## INFO  [13:34:10.527] Applying learner &#39;regr.ranger&#39; on task &#39;sales&#39; (iter 2/3) 
## INFO  [13:34:10.631] Applying learner &#39;regr.ranger&#39; on task &#39;sales&#39; (iter 3/3)</code></pre>
<pre class="r"><code>res$score(msr(&quot;regr.mse&quot;))</code></pre>
<pre><code>##          task task_id             learner  learner_id     resampling
##        &lt;list&gt;  &lt;char&gt;              &lt;list&gt;      &lt;char&gt;         &lt;list&gt;
## 1: &lt;TaskRegr&gt;   sales &lt;LearnerRegrRanger&gt; regr.ranger &lt;ResamplingCV&gt;
## 2: &lt;TaskRegr&gt;   sales &lt;LearnerRegrRanger&gt; regr.ranger &lt;ResamplingCV&gt;
## 3: &lt;TaskRegr&gt;   sales &lt;LearnerRegrRanger&gt; regr.ranger &lt;ResamplingCV&gt;
##    resampling_id iteration prediction regr.mse
##           &lt;char&gt;     &lt;int&gt;     &lt;list&gt;    &lt;num&gt;
## 1:            cv         1     &lt;list&gt; 23837.03
## 2:            cv         2     &lt;list&gt; 22343.27
## 3:            cv         3     &lt;list&gt; 18912.51</code></pre>
<pre class="r"><code>sprintf(&quot;RMSE of the simple ranger: %s&quot;, round(sqrt(res$aggregate()), 2))</code></pre>
<pre><code>## [1] &quot;RMSE of the simple ranger: 147.3&quot;</code></pre>
<p>Often tuning <strong>RandomForest</strong> methods does not increase predictive performances substantially.
If time permits, it can nonetheless lead to improvements and should thus be performed.
In this case, we resort to tune a different kind of model: <strong>Gradient Boosted Decision Trees</strong> from the package <a href="https://cran.r-project.org/package=xgboost">xgboost</a>.</p>
</div>
<div id="a-better-baseline-autotuner" class="section level3">
<h3>A better baseline: <code>AutoTuner</code></h3>
<p>Tuning can often further improve the performance.
In this case, we <em>tune</em> the xgboost learner in order to see whether this can improve performance.
For the <code>AutoTuner</code> we have to specify a <strong>Termination Criterion</strong> (how long the tuning should run) a <strong>Tuner</strong> (which tuning method to use) and a <strong>ParamSet</strong> (which space we might want to search through).
For now we do not use the <strong>zipcode</strong> column, as <a href="https://cran.r-project.org/package=xgboost">xgboost</a> cannot naturally
deal with categorical features.
The <strong>AutoTuner</strong> automatically performs nested cross-validation.</p>
<pre class="r"><code>set.seed(444L)
library(mlr3tuning)
library(paradox)
lrn_xgb = lrn(&quot;regr.xgboost&quot;)

# Define the ParamSet
ps = ParamSet$new(
  params = list(
    ParamDbl$new(id = &quot;eta&quot;, lower = 0.2, upper = .4),
    ParamDbl$new(id = &quot;min_child_weight&quot;, lower = 1, upper = 20),
    ParamDbl$new(id = &quot;subsample&quot;, lower = .7, upper = .8),
    ParamDbl$new(id = &quot;colsample_bytree&quot;,  lower = .9, upper = 1),
    ParamDbl$new(id = &quot;colsample_bylevel&quot;, lower = .5, upper = .7),
    ParamInt$new(id = &quot;nrounds&quot;, lower = 1L, upper = 25)
))

# Define the Terminator
terminator = TerminatorEvaluations$new(10)
cv3 = rsmp(&quot;cv&quot;, folds = 3)
at = AutoTuner$new(lrn_xgb, cv3, measures = msr(&quot;regr.mse&quot;), ps,
  terminator, tuner = TunerRandomSearch, tuner_settings = list())</code></pre>
<pre class="r"><code>res$score(msr(&quot;regr.mse&quot;))
sprintf(&quot;RMSE of the tuned xgboost: %s&quot;, round(sqrt(res$aggregate()), 2))</code></pre>
<p>We can obtain the resulting params in the respective splits by accessing the <a href="https://mlr3.mlr-org.com/reference/ResampleResult.html"><code>ResampleResult</code></a>.</p>
<pre class="r"><code>sapply(res$learners, function(x) x$param_set$values)</code></pre>
<pre><code>## $num.trees
## [1] 15
## 
## $num.trees
## [1] 15
## 
## $num.trees
## [1] 15</code></pre>
<p><strong>NOTE:</strong> To keep runtime low, we only tune parts of the hyperparameter space of <a href="https://cran.r-project.org/package=xgboost">xgboost</a> in this example.
Additionally, we only allow for <span class="math inline">\(10\)</span> random search iterations, which is usually to little for real-world applications.
Nonetheless, we are able to obtain an improved performance when comparing to the <a href="https://cran.r-project.org/package=ranger">ranger</a> model.</p>
<p>In order to further improve our results we have several options:</p>
<ul>
<li>Find or engineer better features</li>
<li>Remove Features to avoid overfitting</li>
<li>Obtain additional data (often prohibitive)</li>
<li>Try more models</li>
<li>Improve the tuning
<ul>
<li>Increase the tuning budget</li>
<li>Enlarge the tuning search space</li>
<li>Use a more efficient tuning algorithm</li>
</ul></li>
<li>Stacking and Ensembles (see <a href="#pipelines">Pipelines</a>)</li>
</ul>
<p>Below we will investigate some of those possibilities and investigate whether this improves performance.</p>
</div>
<div id="engineering-features-mutating-zip-codes" class="section level3">
<h3>Engineering Features: Mutating ZIP-Codes</h3>
<p>In order to better cluster the zip codes, we compute a new feature: <strong>med_price</strong>:
It computes the median price in each zip-code.
This might help our model to improve the prediction.</p>
<pre class="r"><code># Create a new feature and append it to the task
zip_price = house_sales_prediction[, .(med_price = median(price)), by = zipcode]

# Join on the original data to match with original columns, then cbind to the task
tsk$cbind(house_sales_prediction[zip_price, on=&quot;zipcode&quot;][,&quot;med_price&quot;])</code></pre>
<p>Again, we run <code>resample</code> and compute the <strong>RMSE</strong>.</p>
<pre class="r"><code>lrn_ranger = lrn(&quot;regr.ranger&quot;)
res = resample(task = tsk$clone()$filter(train.idx), lrn_ranger, cv3)</code></pre>
<pre><code>## INFO  [13:34:10.798] Applying learner &#39;regr.ranger&#39; on task &#39;sales&#39; (iter 1/3) 
## INFO  [13:34:13.336] Applying learner &#39;regr.ranger&#39; on task &#39;sales&#39; (iter 2/3) 
## INFO  [13:34:16.366] Applying learner &#39;regr.ranger&#39; on task &#39;sales&#39; (iter 3/3)</code></pre>
<pre class="r"><code>res$score(msr(&quot;regr.mse&quot;))</code></pre>
<pre><code>##          task task_id             learner  learner_id     resampling
##        &lt;list&gt;  &lt;char&gt;              &lt;list&gt;      &lt;char&gt;         &lt;list&gt;
## 1: &lt;TaskRegr&gt;   sales &lt;LearnerRegrRanger&gt; regr.ranger &lt;ResamplingCV&gt;
## 2: &lt;TaskRegr&gt;   sales &lt;LearnerRegrRanger&gt; regr.ranger &lt;ResamplingCV&gt;
## 3: &lt;TaskRegr&gt;   sales &lt;LearnerRegrRanger&gt; regr.ranger &lt;ResamplingCV&gt;
##    resampling_id iteration prediction regr.mse
##           &lt;char&gt;     &lt;int&gt;     &lt;list&gt;    &lt;num&gt;
## 1:            cv         1     &lt;list&gt; 22703.84
## 2:            cv         2     &lt;list&gt; 20376.39
## 3:            cv         3     &lt;list&gt; 17782.85</code></pre>
<pre class="r"><code>sprintf(&quot;RMSE of ranger with med_price: %s&quot;, round(sqrt(res$aggregate()), 2))</code></pre>
<pre><code>## [1] &quot;RMSE of ranger with med_price: 142.43&quot;</code></pre>
</div>
<div id="obtaining-a-sparser-model" class="section level3">
<h3>Obtaining a sparser model</h3>
<p>In many cases, we might want to have a sparse model.
For this purpose we can use a <a href="https://mlr3filters.mlr-org.com/reference/Filter.html"><code>mlr3filters::Filter</code></a> implemented in <code>mlr3filters</code>.
This can prevent our learner from overfitting make it easier for humans to interpret models as fewer variables influence the resulting prediction.</p>
<pre class="r"><code>library(mlr3filters)
filter = FilterMRMR$new()$calculate(tsk)
tsk_ftsel = tsk$clone()$select(head(names(filter$scores), 12))</code></pre>
<p>The resulting <strong>RMSE</strong> is slightly higher, and at the same time we only use <span class="math inline">\(12\)</span> features.</p>
<pre class="r"><code>lrn_ranger = lrn(&quot;regr.ranger&quot;)
res = resample(task = tsk_ftsel$clone()$filter(train.idx), lrn_ranger, cv3)</code></pre>
<pre><code>## INFO  [13:34:19.835] Applying learner &#39;regr.ranger&#39; on task &#39;sales&#39; (iter 1/3) 
## INFO  [13:34:21.559] Applying learner &#39;regr.ranger&#39; on task &#39;sales&#39; (iter 2/3) 
## INFO  [13:34:23.365] Applying learner &#39;regr.ranger&#39; on task &#39;sales&#39; (iter 3/3)</code></pre>
<pre class="r"><code>res$score(msr(&quot;regr.mse&quot;))</code></pre>
<pre><code>##          task task_id             learner  learner_id     resampling
##        &lt;list&gt;  &lt;char&gt;              &lt;list&gt;      &lt;char&gt;         &lt;list&gt;
## 1: &lt;TaskRegr&gt;   sales &lt;LearnerRegrRanger&gt; regr.ranger &lt;ResamplingCV&gt;
## 2: &lt;TaskRegr&gt;   sales &lt;LearnerRegrRanger&gt; regr.ranger &lt;ResamplingCV&gt;
## 3: &lt;TaskRegr&gt;   sales &lt;LearnerRegrRanger&gt; regr.ranger &lt;ResamplingCV&gt;
##    resampling_id iteration prediction regr.mse
##           &lt;char&gt;     &lt;int&gt;     &lt;list&gt;    &lt;num&gt;
## 1:            cv         1     &lt;list&gt; 34702.97
## 2:            cv         2     &lt;list&gt; 29051.66
## 3:            cv         3     &lt;list&gt; 25912.50</code></pre>
<pre class="r"><code>sprintf(&quot;RMSE of ranger with filtering: %s&quot;, round(sqrt(res$aggregate()), 2))</code></pre>
<pre><code>## [1] &quot;RMSE of ranger with filtering: 172.88&quot;</code></pre>
</div>
</div>
